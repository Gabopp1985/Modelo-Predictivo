{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATOS LIMPIOS Y PROCESADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para imputar la media o mediana agrupada por COMP_S_NAME\n",
    "def impute_grouped_data(df, column, method='mean'):\n",
    "    if method == 'median':\n",
    "        return df.groupby('COMP_S_NAME')[column].transform(lambda x: x.fillna(x.median()))\n",
    "    elif method == 'linear':\n",
    "        return df[column].interpolate(method='linear')\n",
    "    else:\n",
    "        return df.groupby('COMP_S_NAME')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Cargar las bases de datos\n",
    "produccion_df = pd.read_csv(\"produccion(1).csv\")\n",
    "test_df = pd.read_csv(\"TEST21.csv\")\n",
    "\n",
    "\n",
    "# Eliminar duplicados\n",
    "produccion_df = produccion_df.drop_duplicates()\n",
    "test_df = test_df.drop_duplicates()\n",
    "\n",
    "# Eliminar columnas innecesarias de la tabla de producción\n",
    "produccion_df = produccion_df.drop(columns=['OPERACION', 'ACTIVO', 'BLOQUE_N', 'ROUTE_NAME', 'PAD', 'ANIO', 'MES'])\n",
    "\n",
    "# Eliminar filas con valores faltantes en columnas críticas\n",
    "critical_columns_produccion = ['DIAS', 'POZO', 'COMP_S_NAME']\n",
    "critical_columns_test = ['BSW', 'CASING_PRESS', 'PBHP', 'TUBING_PRESS', 'PIP', 'FLAP', 'SBHP', 'TUBING_TMP', 'AMPS_A', 'PUMP_TMP', 'MOTOR_HZ', 'POWER_KW', 'STAGE_COUNT', 'VOLTAGE', 'INTAKE_DEPTH', 'PUMP_TYPE', 'SALINIDAD', 'INJECTION_VOL', 'RETURN_VOL']\n",
    "\n",
    "produccion_df = produccion_df.dropna(subset=critical_columns_produccion)\n",
    "test_df = test_df.dropna(subset=critical_columns_test)\n",
    "\n",
    "# Imputar valores faltantes en columnas seleccionadas con el método recomendado\n",
    "produccion_df['PC_BPPD'] = impute_grouped_data(produccion_df, 'PC_BPPD', method='median')\n",
    "produccion_df['WAT'] = impute_grouped_data(produccion_df, 'WAT', method='median')\n",
    "produccion_df['GAS'] = impute_grouped_data(produccion_df, 'GAS', method='linear')\n",
    "\n",
    "\n",
    "test_df['GRAV_OIL_API'] = impute_grouped_data(test_df, 'GRAV_OIL_API', method='linear')\n",
    "test_df['GAS_SP_GRAVITY'] = impute_grouped_data(test_df, 'GAS_SP_GRAVITY', method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'produccion_df' y 'test_df' ya están cargados y limpios\n",
    "\n",
    "# Convertir las columnas de fecha al formato correcto\n",
    "produccion_df['DIAS'] = pd.to_datetime(produccion_df['DIAS'], format='%d-%b-%y')\n",
    "test_df['EFF_DT'] = pd.to_datetime(test_df['EFF_DT'])\n",
    "\n",
    "# Combinar los DataFrames usando 'COMP_S_NAME' y las fechas correspondientes\n",
    "combined_df = pd.merge(produccion_df, test_df, left_on=['COMP_S_NAME', 'DIAS'], right_on=['COMP_S_NAME', 'EFF_DT'], how='inner')\n",
    "\n",
    "# Eliminar filas con valores faltantes en columnas críticas\n",
    "critical_columns_produccion = ['DIAS', 'POZO', 'COMP_S_NAME']\n",
    "critical_columns_test = ['BSW', 'CASING_PRESS', 'PBHP', 'TUBING_PRESS', 'PIP', 'FLAP', 'SBHP', 'TUBING_TMP', 'AMPS_A', 'PUMP_TMP', 'MOTOR_HZ', 'POWER_KW', 'STAGE_COUNT', 'VOLTAGE', 'INTAKE_DEPTH', 'PUMP_TYPE', 'SALINIDAD', 'INJECTION_VOL', 'RETURN_VOL']\n",
    "combined_df.dropna(subset=critical_columns_produccion + critical_columns_test, inplace=True)\n",
    "\n",
    "# Imputar valores faltantes en otras columnas con el método recomendado\n",
    "# Asumiendo que la función 'impute_grouped_data' está definida anteriormente en el código\n",
    "combined_df['PC_BPPD'] = impute_grouped_data(combined_df, 'PC_BPPD', method='median')\n",
    "combined_df['WAT'] = impute_grouped_data(combined_df, 'WAT', method='median')\n",
    "combined_df['GAS'] = impute_grouped_data(combined_df, 'GAS', method='linear')\n",
    "combined_df['GRAV_OIL_API'] = impute_grouped_data(combined_df, 'GRAV_OIL_API', method='linear')\n",
    "combined_df['GAS_SP_GRAVITY'] = impute_grouped_data(combined_df, 'GAS_SP_GRAVITY', method='linear')\n",
    "\n",
    "# Extraer características de la columna de fecha 'DIAS'\n",
    "combined_df['year'] = combined_df['DIAS'].dt.year\n",
    "combined_df['month'] = combined_df['DIAS'].dt.month\n",
    "combined_df['day'] = combined_df['DIAS'].dt.day\n",
    "combined_df['dayofweek'] = combined_df['DIAS'].dt.dayofweek\n",
    "\n",
    "# Crear un diccionario para almacenar los mapeos de LabelEncoder\n",
    "le_dict = {}\n",
    "\n",
    "# Aplicar Label Encoding a las columnas seleccionadas incluyendo 'COMP_S_NAME'\n",
    "le = LabelEncoder()\n",
    "columnas_produccion_label = ['POZO', 'COMP_S_NAME']\n",
    "columnas_test_label = ['PUMP_TYPE']\n",
    "\n",
    "# Aplicar Label Encoding a las columnas seleccionadas y guardar los mapeos\n",
    "for columna in columnas_produccion_label + columnas_test_label:\n",
    "    le = LabelEncoder()\n",
    "    combined_df[columna] = le.fit_transform(combined_df[columna])\n",
    "    # Guardar el objeto LabelEncoder en el diccionario\n",
    "    le_dict[columna] = le\n",
    "\n",
    "# Eliminar las columnas de fecha originales 'DIAS' y 'EFF_DT'\n",
    "combined_df.drop(['DIAS', 'EFF_DT'], axis=1, inplace=True)\n",
    "\n",
    "# Lista de variables predictoras incluyendo las nuevas características de fecha\n",
    "predictoras = [\n",
    "    'AMPS_A', 'TUBING_TMP', 'CASING_PRESS', 'PBHP', 'SALINIDAD',\n",
    "    'GAS_SP_GRAVITY', 'STAGE_COUNT', 'POWER_KW', 'TUBING_PRESS',\n",
    "    'PUMP_TMP', 'VOLTAGE', 'INTAKE_DEPTH', 'year', 'month', 'day', 'dayofweek',\n",
    "    'POZO', 'COMP_S_NAME','PUMP_TYPE'\n",
    "]\n",
    "\n",
    "# Verificar si todas las columnas existen en el DataFrame\n",
    "for columna in predictoras:\n",
    "    if columna not in combined_df.columns:\n",
    "        print(f\"La columna {columna} no está en el DataFrame.\")\n",
    "\n",
    "# Definir las características (X) y las variables objetivo (y)\n",
    "X = combined_df[predictoras]  # Variables predictoras\n",
    "y = combined_df[['PC_BPPD', 'WAT', 'GAS']]  # Variables objetivo\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ahora X_train y y_train contienen los datos para entrenar el modelo\n",
    "# X_test y y_test se usarán para probar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISEÑO Y DESARROLLO DEL ARBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor Random Forest MSE: 3564.295141078345\n",
      "Mejor Random Forest R^2: 0.9739631904829306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Mejores hiperparámetros encontrados por GridSearchCV\n",
    "mejores_hiperparametros = {\n",
    "    'bootstrap': True,\n",
    "    'max_depth': None,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 400     \n",
    "}\n",
    "\n",
    "# Inicializar el modelo Random Forest con los mejores hiperparámetros\n",
    "mejor_rf = RandomForestRegressor(\n",
    "    n_estimators=mejores_hiperparametros['n_estimators'],\n",
    "    max_depth=mejores_hiperparametros['max_depth'],\n",
    "    min_samples_split=mejores_hiperparametros['min_samples_split'],\n",
    "    min_samples_leaf=mejores_hiperparametros['min_samples_leaf'],\n",
    "    bootstrap=mejores_hiperparametros['bootstrap'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "mejor_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo con los datos de prueba\n",
    "y_pred_mejor_rf = mejor_rf.predict(X_test)\n",
    "mse_mejor_rf = mean_squared_error(y_test, y_pred_mejor_rf)\n",
    "r2_mejor_rf = r2_score(y_test, y_pred_mejor_rf)\n",
    "\n",
    "print(f'Mejor Random Forest MSE: {mse_mejor_rf}')\n",
    "print(f'Mejor Random Forest R^2: {r2_mejor_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en mejor_rf_modelo_entrenado1.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Asumiendo que 'mejor_rf' es el modelo de Random Forest ya entrenado\n",
    "# Guardar el modelo en la carpeta deseada\n",
    "ruta_guardado = 'mejor_rf_modelo_entrenado1.pkl'\n",
    "joblib.dump(mejor_rf, ruta_guardado)\n",
    "\n",
    "print(f\"Modelo guardado en {ruta_guardado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Función para imputar la media o mediana agrupada por COMP_S_NAME\n",
    "def impute_grouped_data(df, column, method='mean'):\n",
    "    if method == 'median':\n",
    "        return df.groupby('COMP_S_NAME')[column].transform(lambda x: x.fillna(x.median()))\n",
    "    elif method == 'linear':\n",
    "        return df[column].interpolate(method='linear')\n",
    "    else:\n",
    "        return df.groupby('COMP_S_NAME')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Cargar las bases de datos desde Excel para predicciones\n",
    "prod_pred_df = pd.read_excel(\"Base enero 2024.xlsx\", sheet_name='PROD')\n",
    "test_pred_df = pd.read_excel(\"Base enero 2024.xlsx\", sheet_name='PRUEBAS')\n",
    "\n",
    "# Eliminar duplicados\n",
    "prod_pred_df = prod_pred_df.drop_duplicates()\n",
    "test_pred_df = test_pred_df.drop_duplicates()\n",
    "\n",
    "# Convertir las columnas de fecha al formato correcto\n",
    "prod_pred_df['DIAS_PRED'] = pd.to_datetime(prod_pred_df['DIAS'], format='%d-%b-%y')\n",
    "test_pred_df['EFF_DT_PRED'] = pd.to_datetime(test_pred_df['EFF_DT'])\n",
    "\n",
    "# Combinar los DataFrames usando 'COMP_S_NAME' y las fechas correspondientes\n",
    "combined_pred_df = pd.merge(prod_pred_df, test_pred_df, left_on=['COMP_S_NAME', 'DIAS_PRED'], right_on=['COMP_S_NAME', 'EFF_DT_PRED'], how='inner')\n",
    "\n",
    "# Filtrar la columna COMP_S_NAME por los valores específicos\n",
    "valores_filtrados = ['SCH-099TI','SCHAP-482TI','SCHAC-353HI','SCHAO-475HUI','SCHD-251UI']\n",
    "combined_pred_df = combined_pred_df[combined_pred_df['COMP_S_NAME'].isin(valores_filtrados)]\n",
    "\n",
    "# Eliminar filas con valores faltantes en columnas críticas\n",
    "critical_columns_prod_pred = ['ANIO', 'MES', 'DIAS_PRED', 'POZO', 'COMP_S_NAME']  # 'ROUTE_NAME' y 'PAD' no están disponibles\n",
    "critical_columns_test_pred = ['BSW', 'CASING_PRESS', 'PBHP', 'TUBING_PRESS', 'PIP', 'FLAP', 'SBHP', 'TUBING_TMP', 'AMPS_A', 'PUMP_TMP', 'MOTOR_HZ', 'POWER_KW', 'STAGE_COUNT', 'VOLTAGE', 'INTAKE_DEPTH', 'PUMP_TYPE', 'SALINIDAD', 'INJECTION_VOL', 'RETURN_VOL']\n",
    "combined_pred_df.dropna(subset=critical_columns_prod_pred + critical_columns_test_pred, inplace=True)\n",
    "\n",
    "# Imputar valores faltantes en otras columnas con el método recomendado\n",
    "combined_pred_df['PC_BPPD'] = impute_grouped_data(combined_pred_df, 'PC_BPPD', method='median')\n",
    "combined_pred_df['WAT'] = impute_grouped_data(combined_pred_df, 'WAT', method='median')\n",
    "combined_pred_df['GAS'] = impute_grouped_data(combined_pred_df, 'GAS', method='linear')\n",
    "combined_pred_df['GRAV_OIL_API'] = impute_grouped_data(combined_pred_df, 'GRAV_OIL_API', method='linear')\n",
    "combined_pred_df['GAS_SP_GRAVITY'] = impute_grouped_data(combined_pred_df, 'GAS_SP_GRAVITY', method='linear')\n",
    "\n",
    "# Extraer características de la columna de fecha 'DIAS_PRED'\n",
    "combined_pred_df['year'] = combined_pred_df['DIAS_PRED'].dt.year\n",
    "combined_pred_df['month'] = combined_pred_df['DIAS_PRED'].dt.month\n",
    "combined_pred_df['day'] = combined_pred_df['DIAS_PRED'].dt.day\n",
    "combined_pred_df['dayofweek'] = combined_pred_df['DIAS_PRED'].dt.dayofweek\n",
    "\n",
    "\n",
    "# Aplicar Label Encoding a las columnas seleccionadas incluyendo 'COMP_S_NAME'\n",
    "columnas_prod_pred_label = ['POZO', 'COMP_S_NAME']  # 'ROUTE_NAME' y 'PAD' no están disponibles\n",
    "columnas_test_pred_label = ['PUMP_TYPE']\n",
    "\n",
    "# Aplicar Label Encoding a las columnas categóricas usando los mapeos existentes\n",
    "for columna in columnas_prod_pred_label + columnas_test_pred_label:\n",
    "    if columna in le_dict:\n",
    "        le = le_dict[columna]\n",
    "        # Transformar los datos usando el LabelEncoder existente\n",
    "        combined_pred_df[columna] = le.transform(combined_pred_df[columna])\n",
    "    else:\n",
    "        print(f\"La columna {columna} no fue codificada durante el entrenamiento y no se puede transformar.\")\n",
    "\n",
    "# Eliminar las columnas de fecha originales 'DIAS_PRED' y 'EFF_DT_PRED'\n",
    "combined_pred_df.drop(['DIAS_PRED', 'EFF_DT_PRED'], axis=1, inplace=True)\n",
    "\n",
    "# Ahora 'combined_pred_df' está listo para ser utilizado para hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2226.000000</td>\n",
       "      <td>2226.000000</td>\n",
       "      <td>2226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>270.671506</td>\n",
       "      <td>330.616115</td>\n",
       "      <td>82.956867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>228.139333</td>\n",
       "      <td>607.143259</td>\n",
       "      <td>116.723913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.563509</td>\n",
       "      <td>2.026040</td>\n",
       "      <td>0.319247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>126.605145</td>\n",
       "      <td>23.139065</td>\n",
       "      <td>25.855027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>204.746882</td>\n",
       "      <td>81.675095</td>\n",
       "      <td>46.201392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>313.719675</td>\n",
       "      <td>324.033154</td>\n",
       "      <td>82.841809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1609.044769</td>\n",
       "      <td>4231.698896</td>\n",
       "      <td>807.087183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2\n",
       "count  2226.000000  2226.000000  2226.000000\n",
       "mean    270.671506   330.616115    82.956867\n",
       "std     228.139333   607.143259   116.723913\n",
       "min       7.563509     2.026040     0.319247\n",
       "25%     126.605145    23.139065    25.855027\n",
       "50%     204.746882    81.675095    46.201392\n",
       "75%     313.719675   324.033154    82.841809\n",
       "max    1609.044769  4231.698896   807.087183"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_pred_mejor_rf).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "mejor_rf = joblib.load('mejor_rf_modelo_entrenado1.pkl')\n",
    "\n",
    "# Asegúrarse de que 'combined_pred_df' es el DataFrame preparado para la predicción\n",
    "# ...\n",
    "\n",
    "# Lista de variables predictoras basada en el modelo entrenado\n",
    "predictoras_pred = [\n",
    "    'AMPS_A', 'TUBING_TMP', 'CASING_PRESS', 'PBHP', 'SALINIDAD',\n",
    "    'GAS_SP_GRAVITY', 'STAGE_COUNT', 'POWER_KW', 'TUBING_PRESS',\n",
    "    'PUMP_TMP', 'VOLTAGE', 'INTAKE_DEPTH', 'year', 'month', 'day', 'dayofweek',\n",
    "    'POZO', 'COMP_S_NAME','PUMP_TYPE'\n",
    "]\n",
    "\n",
    "# Preparar los datos para la predicción\n",
    "X_pred = combined_pred_df[predictoras_pred]\n",
    "\n",
    "# Realizar las predicciones con el modelo\n",
    "predicciones = mejor_rf.predict(X_pred)\n",
    "\n",
    "# Convertir las predicciones a un DataFrame y añadir las columnas de predicciones al DataFrame original\n",
    "combined_pred_df['PC_BPPD_pred'] = predicciones[:, 0]  # Asumiendo que esta es la primera variable objetivo\n",
    "combined_pred_df['WAT_pred'] = predicciones[:, 1]      # Asumiendo que esta es la segunda variable objetivo\n",
    "combined_pred_df['GAS_pred'] = predicciones[:, 2]      # Asumiendo que esta es la tercera variable objetivo\n",
    "\n",
    "# Revertir el Label Encoding para las columnas categóricas\n",
    "for columna in ['POZO', 'COMP_S_NAME', 'PUMP_TYPE']:\n",
    "    if columna in le_dict:\n",
    "        le = le_dict[columna]\n",
    "        combined_pred_df[columna] = le.inverse_transform(combined_pred_df[columna])\n",
    "\n",
    "# Guardar los resultados en un nuevo archivo Excel\n",
    "combined_pred_df.to_excel(\"Resultados_Predicciones.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
